{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHgmxWG_7lnE"
   },
   "source": [
    "# Введение в анализ данных\n",
    "## НИУ ВШЭ, 2019-2020 учебный год\n",
    "\n",
    "### Домашнее задание №3\n",
    "\n",
    "Задание выполнила: Анастасия Успенская\n",
    "\n",
    "### Общая информация\n",
    "\n",
    "__Дата выдачи:__ 06.04.2020\n",
    "\n",
    "__Дедлайн:__ 23:59 20.04.2020\n",
    "\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Оценка за ДЗ вычисляется по следующей формуле:\n",
    "\n",
    "$$\n",
    "min(\\text{points}, 18)  \\times 10 / 18,\n",
    "$$\n",
    "\n",
    "где points — количество баллов за обязательную часть, которое вы набрали. Максимальное число баллов, которое можно получить за обязательную часть — 18, за каждые полтора балла сверху вы получите 1 бонусный балл (максимум 2). Также вы можете использовать бонусные баллы, которые накопили ранее.\n",
    "\n",
    "За сдачу задания позже срока на итоговую оценку за задание накладывается штраф в размере 1 балл в день, но получить отрицательную оценку нельзя.\n",
    "\n",
    "__Внимание!__ Домашнее задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов.\n",
    "\n",
    "### Формат сдачи\n",
    "\n",
    "Загрузка файлов с решениями происходит в системе [Anytask](https://anytask.org/).\n",
    "\n",
    "Инвайт для группы ИАД-6: rd5CNrr\n",
    "\n",
    "Перед отправкой перезагрузите ноутбук и проверьте, что все ячейки могут быть последовательно выполнены. Ноутбук должен запускаться с использованием python 3.6+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztx03xvr9T95"
   },
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVrrwTJNjuDt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "colab_type": "code",
    "id": "_VMchexbjjTh",
    "outputId": "c1f66a1f-8851-42f3-e7e5-6c48d3c24707"
   },
   "outputs": [],
   "source": [
    "# Качаем датасет\n",
    "\n",
    "!wget https://www.dropbox.com/s/tg55q9mrziroyrs/train_subset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvXKae8q9nn-"
   },
   "source": [
    "### Данные\n",
    "\n",
    "Мы имеем дело с данными с торговой платформы Avito.\n",
    "Для каждого товара представлены следующие параметры:\n",
    " - title\n",
    " - description\n",
    " - Category_name\n",
    " - Category\n",
    "\n",
    "Имеется информация об объектах 50 классов.\n",
    "Задача: по новым объектам (title, description) предсказать Category.\n",
    "(Очевидно, что параметр Category_name для предсказания классов использовать нельзя)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "BqEuoDhqNgoa",
    "outputId": "b345f049-ae77-4d1b-a25f-4d4f447e63d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>Category_name</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>382220</th>\n",
       "      <td>Прихожая</td>\n",
       "      <td>В хорошем состоянии. Торг</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397529</th>\n",
       "      <td>Кордиант 215/55/16 Летние</td>\n",
       "      <td>Кордиант 215/55/16 Летние/\\n /\\nАртикул: 1737l...</td>\n",
       "      <td>Запчасти и аксессуары</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584569</th>\n",
       "      <td>Стол</td>\n",
       "      <td>Стол, 2 рабочих места . Стол серого цвета, в д...</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513100</th>\n",
       "      <td>Комбинезон</td>\n",
       "      <td>Размер-42/44</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091886</th>\n",
       "      <td>Ветровка</td>\n",
       "      <td>На 2 года</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title  \\\n",
       "id                                   \n",
       "382220                    Прихожая   \n",
       "397529   Кордиант 215/55/16 Летние   \n",
       "584569                        Стол   \n",
       "2513100                 Комбинезон   \n",
       "1091886                   Ветровка   \n",
       "\n",
       "                                               description  \\\n",
       "id                                                           \n",
       "382220                           В хорошем состоянии. Торг   \n",
       "397529   Кордиант 215/55/16 Летние/\\n /\\nАртикул: 1737l...   \n",
       "584569   Стол, 2 рабочих места . Стол серого цвета, в д...   \n",
       "2513100                                       Размер-42/44   \n",
       "1091886                                          На 2 года   \n",
       "\n",
       "                     Category_name  Category  \n",
       "id                                            \n",
       "382220           Мебель и интерьер        20  \n",
       "397529       Запчасти и аксессуары        10  \n",
       "584569           Мебель и интерьер        20  \n",
       "2513100  Одежда, обувь, аксессуары        27  \n",
       "1091886     Детская одежда и обувь        29  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"https://www.dropbox.com/s/tg55q9mrziroyrs/train_subset.csv?dl=1\", index_col='id')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Kg8iPp7fiwGh",
    "outputId": "96ed00ed-b63b-4478-f2d4-66bda1110b5c"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1hvzAMETU2d"
   },
   "outputs": [],
   "source": [
    "X = data[['title', 'description']].to_numpy()\n",
    "y = data['Category'].to_numpy()\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMYU7zZw_cw-"
   },
   "source": [
    "Сразу разделим выборку на train и test.\n",
    "Никакие данные из test для обучения использовать нельзя!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fia4_3vNprp"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "qDR8LtTJUIGt",
    "outputId": "fd4d5b55-a023-4129-9ff5-a6e8e24db915"
   },
   "outputs": [],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-ZEdlEGAXTD"
   },
   "source": [
    "### Токенизация (1 балл)\n",
    "\n",
    "\n",
    "Токенизация -- разбиение текста на мелкие части, которые можно обработать машинными методами.\n",
    "Можно использовать разные алгоритмы токенизации.\n",
    "Можете использовать WordPunctTokenizer или подобрать какой-то другой, если считаете, что он лучше подойдет для этой задачи.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Здраствуйте. Я, Кирилл. Хотел бы чтобы вы сделали игру, 3Д-экшон суть такова...\n",
      "after: ['здраствуйте', '.', 'я', ',', 'кирилл', '.', 'хотел', 'бы', 'чтобы', 'вы', 'сделали', 'игру', ',', '3д', '-', 'экшон', 'суть', 'такова', '...']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "text = 'Здраствуйте. Я, Кирилл. Хотел бы чтобы вы сделали игру, 3Д-экшон суть такова...'\n",
    "\n",
    "print(\"before:\", text,)\n",
    "print(\"after:\", tokenizer.tokenize(text.lower()),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_RYBKC26o1X"
   },
   "source": [
    "__Задание:__ реализуйте функцию ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "O9VgNlZ1Qy3o",
    "outputId": "59ef3a75-008e-47c5-fba8-a319eba13ef4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'здраствуйте . я , кирилл . хотел бы чтобы вы сделали игру , 3д - экшон суть такова ...'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Данная функция принимает на вход текст, \n",
    "    а возвращает тот же текст, но с пробелами между каждым токеном\n",
    "    \"\"\"\n",
    "    return ' '.join(tokenizer.tokenize(text.lower()))\n",
    "preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert preprocess(text) == 'здраствуйте . я , кирилл . хотел бы чтобы вы сделали игру , 3д - экшон суть такова ...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ токенизируйте title и description в train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "for x in chain(X_train, X_test):\n",
    "    x[0] = preprocess(x[0])\n",
    "    x[1] = preprocess(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDnDSWwFDwFo"
   },
   "outputs": [],
   "source": [
    "assert X_train[10][1] == 'продам иж планета 3 , 76 год , ( стоит на старом учёте , документы утеряны ) на ходу , хорошее состояние , все интересующие вопросы по телефону ( с родной коляской на 3 тысячи дороже ) . торга не будет .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[10][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlIITUk0AsmS"
   },
   "source": [
    "### BOW (3 балла)\n",
    "\n",
    "Один из традиционных подходов -- построение bag of words.\n",
    "\n",
    "Метод состоит в следующем:\n",
    "\n",
    " - Составить словарь самых часто встречающихся слов в train data\n",
    " - Для каждого примера из train посчитать, сколько раз каждое слово из словаря в нём встречается\n",
    "\n",
    "\n",
    " В sklearn есть CountVectorizer, но в этом задании его использовать нельзя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMKUttDWIF92"
   },
   "source": [
    "__Задание:__ создайте словарь, где в соответствии каждому токену стоит количество раз, которое оно встретилось в X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "senlist = [j for sub in X_train for j in sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import dropwhile\n",
    "import re\n",
    "for i in range(len(senlist)):\n",
    "    senlist [i]= re.sub(r'\\W|\\b\\d+\\b',' ', senlist [i]) # Убираем пунктуацию и цифры, но оставляем вещи типа 128gb\n",
    "    senlist [i] = re.sub(r'\\s+',' ', senlist [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senlist[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {}\n",
    "for i in senlist:\n",
    "    freq = word_tokenize(i)\n",
    "    for token in freq:\n",
    "        if token not in tokens.keys():\n",
    "            tokens[token] = 1\n",
    "        else:\n",
    "            tokens[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'сапоги': 454,\n",
       " 'размер': 3320,\n",
       " 'новые': 1872,\n",
       " 'светильники': 26,\n",
       " 'потолочный': 8,\n",
       " 'swarovski': 9,\n",
       " 'потолочные': 8,\n",
       " 'штук': 135,\n",
       " 'цена': 2379,\n",
       " 'за': 3425,\n",
       " 'штуку': 134,\n",
       " 'в': 28337,\n",
       " 'эксплуатации': 157,\n",
       " 'года': 1012,\n",
       " 'продаются': 152,\n",
       " 'связи': 322,\n",
       " 'со': 853,\n",
       " 'сменой': 10,\n",
       " 'интерьера': 39,\n",
       " 'квартире': 261,\n",
       " 'iphone': 465,\n",
       " 'plus': 163,\n",
       " '128gb': 25,\n",
       " 'red': 34,\n",
       " 'красный': 121,\n",
       " 'наличии': 2172,\n",
       " 'данная': 149,\n",
       " 'только': 1129,\n",
       " 'для': 9627,\n",
       " 'подписчиков': 2,\n",
       " 'instagram': 39,\n",
       " 'iqmac': 1,\n",
       " 'новый': 1626,\n",
       " 'айфон': 37,\n",
       " 'это': 744,\n",
       " 'элегантный': 14,\n",
       " 'и': 21714,\n",
       " 'мощный': 53,\n",
       " 'смартфон': 70,\n",
       " 'который': 200,\n",
       " 'готов': 82,\n",
       " 'полной': 71,\n",
       " 'мере': 9,\n",
       " 'раскрыть': 4,\n",
       " 'возможности': 70,\n",
       " 'ios': 37,\n",
       " 'аппарат': 117,\n",
       " 'с': 12860,\n",
       " 'ядерным': 1,\n",
       " 'процессором': 5,\n",
       " 'а10': 2,\n",
       " 'гб': 213,\n",
       " 'озу': 24,\n",
       " 'легкостью': 13,\n",
       " 'решает': 4,\n",
       " 'самые': 173,\n",
       " 'ресурсоемкие': 1,\n",
       " 'задачи': 25,\n",
       " 'позволяя': 5,\n",
       " 'наслаждаться': 15,\n",
       " 'быстродействием': 1,\n",
       " 'тяжелых': 8,\n",
       " 'приложений': 4,\n",
       " 'игр': 102,\n",
       " 'на': 19465,\n",
       " 'дюймовом': 2,\n",
       " 'дисплее': 8,\n",
       " 'получил': 6,\n",
       " 'экран': 155,\n",
       " 'как': 1468,\n",
       " 'у': 3855,\n",
       " 'ipad': 40,\n",
       " 'pro': 222,\n",
       " 'так': 1406,\n",
       " 'что': 1003,\n",
       " 'картинка': 8,\n",
       " 'теперь': 58,\n",
       " 'соответствует': 77,\n",
       " 'кинематографическому': 1,\n",
       " 'стандарту': 11,\n",
       " 'пион': 4,\n",
       " 'ирис': 1,\n",
       " 'ромашка': 7,\n",
       " 'рассада': 5,\n",
       " 'куст': 10,\n",
       " 'р': 1987,\n",
       " 'более': 1125,\n",
       " 'шт': 1077,\n",
       " 'саженец': 1,\n",
       " 'корень': 1,\n",
       " '100р': 68,\n",
       " 'растут': 16,\n",
       " 'нас': 830,\n",
       " 'лет': 1019,\n",
       " 'розовые': 27,\n",
       " 'бордовые': 6,\n",
       " 'белые': 78,\n",
       " 'фото': 1978,\n",
       " 'цветы': 58,\n",
       " '2018г': 19,\n",
       " 'п': 272,\n",
       " 'зубчаниновка': 1,\n",
       " 'либо': 271,\n",
       " 'пл': 51,\n",
       " 'революции': 7,\n",
       " 'есть': 2906,\n",
       " 'ирисы': 2,\n",
       " 'клубника': 11,\n",
       " 'боярышник': 1,\n",
       " 'ирга': 3,\n",
       " 'кофта': 140,\n",
       " 'состояние': 2666,\n",
       " 'отличное': 976,\n",
       " 'к': 2681,\n",
       " 'квартира': 1282,\n",
       " 'м²': 1159,\n",
       " 'эт': 762,\n",
       " 'продаётся': 244,\n",
       " 'уютная': 90,\n",
       " 'тёплая': 144,\n",
       " 'экологически': 59,\n",
       " 'чистом': 22,\n",
       " 'районе': 224,\n",
       " 'города': 583,\n",
       " 'рядом': 506,\n",
       " 'сосновый': 9,\n",
       " 'бор': 19,\n",
       " 'всегда': 582,\n",
       " 'чистый': 133,\n",
       " 'воздух': 29,\n",
       " 'дом': 1015,\n",
       " 'г': 1348,\n",
       " 'хорошие': 108,\n",
       " 'соседи': 106,\n",
       " 'площадке': 24,\n",
       " 'е': 131,\n",
       " 'квартиры': 241,\n",
       " 'развитая': 63,\n",
       " 'инфраструктура': 130,\n",
       " 'шаговой': 190,\n",
       " 'доступности': 237,\n",
       " 'поликлиника': 57,\n",
       " 'школа': 223,\n",
       " 'тк': 482,\n",
       " 'орбита': 2,\n",
       " 'вещевой': 3,\n",
       " 'рынок': 75,\n",
       " 'хорошем': 1479,\n",
       " 'состоянии': 3981,\n",
       " 'подходит': 440,\n",
       " 'под': 1468,\n",
       " 'ипотеку': 61,\n",
       " 'долгов': 14,\n",
       " 'обременений': 27,\n",
       " 'перепланировке': 2,\n",
       " 'нет': 863,\n",
       " 'натяжные': 159,\n",
       " 'потолки': 212,\n",
       " 'ванной': 63,\n",
       " 'комнате': 64,\n",
       " 'стены': 102,\n",
       " 'выполнены': 50,\n",
       " 'из': 3137,\n",
       " 'влагостойких': 1,\n",
       " 'стеновых': 2,\n",
       " 'панелей': 16,\n",
       " 'возможен': 542,\n",
       " 'обмен': 613,\n",
       " 'квартиру': 271,\n",
       " 'магнитогорске': 1,\n",
       " 'торг': 1233,\n",
       " 'платье': 1341,\n",
       " 'новое': 509,\n",
       " 'размера': 232,\n",
       " 'красивого': 19,\n",
       " 'темно': 170,\n",
       " 'синего': 46,\n",
       " 'цвета': 543,\n",
       " 'трикотажной': 13,\n",
       " 'ткани': 117,\n",
       " 'вискоза': 52,\n",
       " 'эластина': 2,\n",
       " 'а': 1433,\n",
       " 'образного': 1,\n",
       " 'силуэта': 14,\n",
       " 'рукавом': 48,\n",
       " 'длинна': 72,\n",
       " 'по': 10000,\n",
       " 'спинке': 102,\n",
       " '113см': 1,\n",
       " 'ваз': 231,\n",
       " 'samara': 19,\n",
       " 'продам': 2677,\n",
       " 'владельца': 28,\n",
       " 'птс': 189,\n",
       " 'оригинал': 1323,\n",
       " 'машина': 324,\n",
       " 'родной': 106,\n",
       " 'краске': 41,\n",
       " 'дтп': 84,\n",
       " 'никогда': 39,\n",
       " 'не': 6045,\n",
       " 'была': 120,\n",
       " 'днище': 15,\n",
       " 'целое': 23,\n",
       " 'ржавое': 2,\n",
       " 'ходовой': 27,\n",
       " 'нареканий': 38,\n",
       " 'сел': 25,\n",
       " 'поехал': 30,\n",
       " 'имеется': 408,\n",
       " 'музыка': 78,\n",
       " 'сигнализация': 140,\n",
       " 'комплекта': 161,\n",
       " 'ключей': 86,\n",
       " 'птф': 66,\n",
       " 'передние': 203,\n",
       " 'стеклоподъемники': 27,\n",
       " 'небольшой': 192,\n",
       " 'при': 2111,\n",
       " 'осмотре': 64,\n",
       " 'интересует': 61,\n",
       " 'наушники': 204,\n",
       " 'блутус': 1,\n",
       " 'долго': 70,\n",
       " 'держат': 16,\n",
       " 'заряд': 23,\n",
       " 'часов': 252,\n",
       " 'можно': 1855,\n",
       " 'больше': 374,\n",
       " 'средней': 25,\n",
       " 'громкости': 12,\n",
       " 'выжать': 1,\n",
       " 'них': 105,\n",
       " 'вкладыши': 22,\n",
       " 'пальто': 449,\n",
       " 'tommy': 30,\n",
       " 'hilfiger': 26,\n",
       " 'нового': 261,\n",
       " 'промахнулась': 4,\n",
       " 'размером': 61,\n",
       " 'до': 4541,\n",
       " 'градусов': 96,\n",
       " 'возможна': 789,\n",
       " 'пересылка': 33,\n",
       " 'почте': 69,\n",
       " 'иж': 6,\n",
       " 'планета': 5,\n",
       " 'год': 739,\n",
       " 'стоит': 233,\n",
       " 'старом': 3,\n",
       " 'учёте': 9,\n",
       " 'документы': 330,\n",
       " 'утеряны': 5,\n",
       " 'ходу': 64,\n",
       " 'хорошее': 683,\n",
       " 'все': 3382,\n",
       " 'интересующие': 71,\n",
       " 'вопросы': 536,\n",
       " 'телефону': 861,\n",
       " 'коляской': 6,\n",
       " 'тысячи': 19,\n",
       " 'дороже': 78,\n",
       " 'торга': 257,\n",
       " 'будет': 371,\n",
       " 'брендовые': 19,\n",
       " 'джинсы': 306,\n",
       " 'zara': 115,\n",
       " 'бирками': 43,\n",
       " 'стояли': 57,\n",
       " '2600р': 5,\n",
       " 'отдам': 363,\n",
       " 'даром': 27,\n",
       " 'почти': 235,\n",
       " 'бесплатно': 611,\n",
       " 'пишите': 876,\n",
       " 'мне': 131,\n",
       " 'сюда': 54,\n",
       " 'утеплённые': 40,\n",
       " 'мальчика': 414,\n",
       " '12лет': 6,\n",
       " 'синие': 52,\n",
       " 'пояс': 102,\n",
       " 'регулируются': 33,\n",
       " 'пуговках': 3,\n",
       " 'посадка': 24,\n",
       " 'чуть': 79,\n",
       " 'пр': 258,\n",
       " 'во': 516,\n",
       " 'турция': 93,\n",
       " 'см': 2774,\n",
       " 'длина': 946,\n",
       " 'брюк92': 1,\n",
       " 'отличном': 1577,\n",
       " 'часы': 334,\n",
       " 'lol': 17,\n",
       " 'конечно': 39,\n",
       " 'новых': 296,\n",
       " 'полностью': 492,\n",
       " 'рабочие': 87,\n",
       " 'подарили': 51,\n",
       " 'но': 781,\n",
       " 'ребёнок': 59,\n",
       " 'играет': 12,\n",
       " 'куклы': 31,\n",
       " 'куклу': 5,\n",
       " 'магазине': 667,\n",
       " 'кукла': 32,\n",
       " 'интересная': 14,\n",
       " 'рубашка': 208,\n",
       " 'принтом': 12,\n",
       " 'поп': 2,\n",
       " 'арт': 184,\n",
       " 'творческих': 5,\n",
       " 'личностей': 3,\n",
       " 'нежная': 11,\n",
       " 'смесовой': 2,\n",
       " 'хлопок': 251,\n",
       " 'шелк': 17,\n",
       " 'стиле': 81,\n",
       " 'bohoshik': 1,\n",
       " 'маркировка': 184,\n",
       " '4xl': 4,\n",
       " 'подойдет': 198,\n",
       " 'от': 4482,\n",
       " 'включительно': 8,\n",
       " 'отлично': 244,\n",
       " 'смотрится': 108,\n",
       " 'жилетами': 1,\n",
       " 'брюки': 312,\n",
       " 'носить': 140,\n",
       " 'ремень': 159,\n",
       " 'остальные': 185,\n",
       " 'модели': 353,\n",
       " 'блузонов': 1,\n",
       " 'брючки': 17,\n",
       " 'смотрите': 414,\n",
       " 'профиле': 239,\n",
       " 'двигатель': 500,\n",
       " '9tdi': 9,\n",
       " 'asv': 2,\n",
       " 'шкода': 104,\n",
       " 'октавия': 42,\n",
       " 'тур': 13,\n",
       " 'tdi': 34,\n",
       " 'пробег': 209,\n",
       " '75000миль': 1,\n",
       " 'привезен': 20,\n",
       " 'англии': 150,\n",
       " 'выпуска': 241,\n",
       " 'москве': 583,\n",
       " 'стоимость': 569,\n",
       " 'без': 3258,\n",
       " 'навесного': 27,\n",
       " 'оборудования': 181,\n",
       " 'мы': 1373,\n",
       " 'продаём': 73,\n",
       " 'контрактные': 63,\n",
       " 'двигатели': 37,\n",
       " 'кпп': 180,\n",
       " 'привезенными': 4,\n",
       " 'европы': 326,\n",
       " 'или': 2631,\n",
       " 'весь': 493,\n",
       " 'товар': 509,\n",
       " 'проходит': 66,\n",
       " 'проверку': 266,\n",
       " 'качество': 504,\n",
       " 'товара': 800,\n",
       " 'пригодное': 6,\n",
       " 'дальнейшего': 7,\n",
       " 'использования': 107,\n",
       " 'покупке': 384,\n",
       " 'предоставим': 165,\n",
       " 'вам': 886,\n",
       " 'договор': 229,\n",
       " 'купли': 171,\n",
       " 'продажи': 342,\n",
       " 'таможенную': 8,\n",
       " 'декларацию': 8,\n",
       " 'купленный': 10,\n",
       " 'агрегат': 37,\n",
       " 'гарантию': 167,\n",
       " 'любую': 201,\n",
       " 'автозапчасть': 8,\n",
       " 'календарных': 11,\n",
       " 'дней': 481,\n",
       " 'момента': 101,\n",
       " 'получения': 112,\n",
       " 'работаем': 958,\n",
       " 'регионами': 120,\n",
       " 'отправка': 901,\n",
       " 'транспортной': 475,\n",
       " 'компанией': 313,\n",
       " 'подробности': 152,\n",
       " 'уточнить': 44,\n",
       " 'whatsapp': 442,\n",
       " 'viber': 395,\n",
       " 'постоянным': 213,\n",
       " 'клиентам': 222,\n",
       " 'сто': 113,\n",
       " 'оптовикам': 17,\n",
       " 'скидки': 491,\n",
       " 'ежедневно': 349,\n",
       " 'оказываем': 25,\n",
       " 'услуги': 340,\n",
       " 'удалению': 4,\n",
       " 'программному': 6,\n",
       " 'отключению': 6,\n",
       " 'сажевого': 4,\n",
       " 'фильтра': 88,\n",
       " 'dpf': 5,\n",
       " 'fap': 3,\n",
       " 'клапана': 21,\n",
       " 'егр': 19,\n",
       " 'вихревых': 3,\n",
       " 'заслонок': 8,\n",
       " 'дроссель': 15,\n",
       " 'ных': 3,\n",
       " 'катализатора': 5,\n",
       " 'изменение': 20,\n",
       " 'евро': 90,\n",
       " 'норм': 15,\n",
       " 'отключение': 14,\n",
       " 'adblue': 4,\n",
       " 'чип': 21,\n",
       " 'тюнинг': 42,\n",
       " 'chiptuning': 3,\n",
       " 'chip': 6,\n",
       " 'tuning': 17,\n",
       " 'двигателя': 478,\n",
       " 'программное': 5,\n",
       " 'увеличение': 16,\n",
       " 'мощности': 45,\n",
       " 'уменьшение': 3,\n",
       " 'расхода': 6,\n",
       " 'топлива': 52,\n",
       " 'ремонт': 809,\n",
       " 'выхлопных': 3,\n",
       " 'систем': 105,\n",
       " 'установка': 501,\n",
       " 'пламегасителя': 3,\n",
       " 'компьютерная': 14,\n",
       " 'диагностика': 76,\n",
       " 'шиномонтаж': 218,\n",
       " 'замена': 280,\n",
       " 'грм': 36,\n",
       " 'автоэлектрика': 3,\n",
       " 'заслонка': 57,\n",
       " 'дроссельная': 41,\n",
       " 'toyota': 413,\n",
       " 'rav': 23,\n",
       " 'тойота': 178,\n",
       " 'рав': 20,\n",
       " '4х4': 37,\n",
       " 'мех': 380,\n",
       " 'данные': 167,\n",
       " 'двигателю': 134,\n",
       " 'объем': 364,\n",
       " 'дизель': 96,\n",
       " '1cdftv': 1,\n",
       " 'описание': 288,\n",
       " 'сторона': 164,\n",
       " 'направление': 130,\n",
       " 'кузов': 404,\n",
       " 'джип': 51,\n",
       " 'дверный': 35,\n",
       " 'оригинальный': 517,\n",
       " 'номер': 1089,\n",
       " 'артикул': 773,\n",
       " 'сайте': 584,\n",
       " 'переплат': 173,\n",
       " 'перекупщикам': 131,\n",
       " 'компания': 394,\n",
       " 'моторлэнд': 125,\n",
       " 'является': 270,\n",
       " 'прямым': 129,\n",
       " 'продавцом': 137,\n",
       " 'запчастей': 1065,\n",
       " 'разборок': 133,\n",
       " 'сша': 218,\n",
       " 'японии': 276,\n",
       " 'канады': 130,\n",
       " 'австралии': 129,\n",
       " 'доставка': 2840,\n",
       " 'бесплатная': 498,\n",
       " 'предоплаты': 217,\n",
       " 'где': 273,\n",
       " 'находятся': 202,\n",
       " 'наши': 489,\n",
       " 'магазины': 439,\n",
       " 'москва': 484,\n",
       " 'брянск': 144,\n",
       " 'владимир': 147,\n",
       " 'великий': 131,\n",
       " 'новгород': 322,\n",
       " 'иваново': 128,\n",
       " 'кострома': 130,\n",
       " 'нижний': 228,\n",
       " 'псков': 129,\n",
       " 'санкт': 351,\n",
       " 'петербург': 211,\n",
       " 'смоленск': 136,\n",
       " 'тверь': 134,\n",
       " 'тула': 132,\n",
       " 'ярославль': 135,\n",
       " 'рязань': 136,\n",
       " 'калуга': 137,\n",
       " 'астрахань': 131,\n",
       " 'белгород': 147,\n",
       " 'волгоград': 148,\n",
       " 'воронеж': 160,\n",
       " 'краснодар': 203,\n",
       " 'курск': 132,\n",
       " 'липецк': 133,\n",
       " 'орел': 130,\n",
       " 'ростов': 178,\n",
       " 'дону': 184,\n",
       " 'саратов': 139,\n",
       " 'регионы': 1031,\n",
       " 'транспортными': 716,\n",
       " 'компаниями': 742,\n",
       " 'полный': 640,\n",
       " 'пакет': 316,\n",
       " 'документов': 277,\n",
       " 'копию': 125,\n",
       " 'грузовой': 171,\n",
       " 'таможенной': 125,\n",
       " 'декларации': 129,\n",
       " 'карту': 295,\n",
       " 'тестирования': 129,\n",
       " 'цены': 898,\n",
       " 'наличие': 609,\n",
       " 'уточняйте': 759,\n",
       " 'менеджера': 245,\n",
       " 'звоните': 1858,\n",
       " 'указанный': 144,\n",
       " 'разделе': 164,\n",
       " 'контакты': 183,\n",
       " 'оптовым': 169,\n",
       " 'простоты': 121,\n",
       " 'подбора': 121,\n",
       " 'запчасти': 1717,\n",
       " 'сообщите': 132,\n",
       " 'менеджеру': 173,\n",
       " 'артикула': 124,\n",
       " 'ботильоны': 111,\n",
       " 'натуральная': 424,\n",
       " 'кожа': 677,\n",
       " '36р': 6,\n",
       " 'honda': 133,\n",
       " 'hr': 9,\n",
       " 'v': 265,\n",
       " 'факту': 86,\n",
       " 'два': 549,\n",
       " 'обслуживалась': 10,\n",
       " 'специализированном': 20,\n",
       " 'сервисе': 102,\n",
       " 'записи': 50,\n",
       " 'много': 546,\n",
       " 'сделано': 43,\n",
       " 'установлена': 82,\n",
       " 'хорошая': 122,\n",
       " 'дорогая': 23,\n",
       " 'возможностью': 62,\n",
       " 'воспроизводить': 5,\n",
       " 'всех': 415,\n",
       " 'носителей': 5,\n",
       " 'устройств': 32,\n",
       " 'связь': 64,\n",
       " 'телефоном': 19,\n",
       " 'зимняя': 237,\n",
       " 'резина': 283,\n",
       " 'стул': 87,\n",
       " 'барный': 7,\n",
       " 'б': 2266,\n",
       " 'сиденье': 96,\n",
       " 'заменено': 13,\n",
       " 'рабочая': 96,\n",
       " 'часть': 216,\n",
       " 'подножки': 27,\n",
       " 'стёрлась': 1,\n",
       " 'краска': 38,\n",
       " 'босоножки': 182,\n",
       " 'женские': 208,\n",
       " 'силиконовые': 22,\n",
       " 'стразами': 43,\n",
       " 'мягко': 8,\n",
       " 'ложится': 4,\n",
       " 'фигуре': 46,\n",
       " 'книги': 102,\n",
       " 'книга': 141,\n",
       " 'тетрадь': 28,\n",
       " 'abs': 123,\n",
       " 'volvo': 155,\n",
       " 'xc': 14,\n",
       " 'очень': 2064,\n",
       " 'большой': 820,\n",
       " 'ассортимент': 739,\n",
       " 'заменитель': 4,\n",
       " 'доступным': 27,\n",
       " 'ценам': 216,\n",
       " 'области': 300,\n",
       " 'деловые': 328,\n",
       " 'линии': 380,\n",
       " 'пек': 7,\n",
       " 'автобусом': 29,\n",
       " 'покупка': 94,\n",
       " 'автомобилей': 529,\n",
       " 'любом': 190,\n",
       " 'гарантия': 1479,\n",
       " 'установку': 163,\n",
       " 'новая': 1159,\n",
       " 'фоторамка': 5,\n",
       " 'новую': 139,\n",
       " 'фоторамку': 3,\n",
       " 'м': 1853,\n",
       " 'новой': 186,\n",
       " 'вещи': 278,\n",
       " 'одето': 45,\n",
       " 'один': 770,\n",
       " 'раз': 863,\n",
       " 'мероприятие': 21,\n",
       " 'массаж': 22,\n",
       " 'дому': 55,\n",
       " 'лечебный': 6,\n",
       " 'медицинский': 15,\n",
       " 'класси': 1,\n",
       " 'й': 113,\n",
       " 'придет': 6,\n",
       " 'специалист': 28,\n",
       " 'медицинским': 2,\n",
       " 'образованием': 4,\n",
       " 'пуховик': 267,\n",
       " 'осень': 423,\n",
       " 'зима': 253,\n",
       " 'фотоснимков': 2,\n",
       " 'петродворец': 1,\n",
       " 'сама': 43,\n",
       " 'упаковка': 113,\n",
       " 'немного': 206,\n",
       " 'потрепана': 4,\n",
       " 'внутри': 397,\n",
       " 'написаны': 3,\n",
       " 'фамилии': 3,\n",
       " 'бейсболка': 13,\n",
       " 'new': 62,\n",
       " 'era': 5,\n",
       " 'подошел': 34,\n",
       " 'куплена': 68,\n",
       " 'мексике': 1,\n",
       " 'сделана': 48,\n",
       " 'же': 904,\n",
       " 'чиносами': 1,\n",
       " 'увы': 6,\n",
       " 'куртка': 929,\n",
       " 'пуху': 12,\n",
       " 'межсезонье': 12,\n",
       " 'красивую': 16,\n",
       " 'куртку': 115,\n",
       " 'женскую': 16,\n",
       " 'носилась': 22,\n",
       " 'зимой': 97,\n",
       " 'воротник': 64,\n",
       " 'кролик': 14,\n",
       " 'отстегивается': 83,\n",
       " 'цвет': 1174,\n",
       " 'васильковый': 1,\n",
       " 'может': 413,\n",
       " 'изменен': 1,\n",
       " 'примеркой': 25,\n",
       " 'договоримся': 76,\n",
       " 'continental': 54,\n",
       " '2э': 1,\n",
       " 'r17': 91,\n",
       " 'sportcontact': 2,\n",
       " 'комплект': 1168,\n",
       " 'единицу': 8,\n",
       " 'производитель': 267,\n",
       " 'резины': 85,\n",
       " 'модель': 572,\n",
       " 'сезонах': 4,\n",
       " 'остаток': 38,\n",
       " 'протектора': 42,\n",
       " 'т': 765,\n",
       " 'возможно': 243,\n",
       " 'даём': 17,\n",
       " 'установки': 111,\n",
       " 'смотрие': 1,\n",
       " 'ещё': 271,\n",
       " 'предложения': 102,\n",
       " 'нашем': 822,\n",
       " 'авито': 612,\n",
       " 'игра': 74,\n",
       " 'ps4': 60,\n",
       " 'mafia': 6,\n",
       " 'iii': 67,\n",
       " 'гик': 1,\n",
       " 'шоп': 6,\n",
       " 'нормандия': 1,\n",
       " 'тц': 190,\n",
       " 'пионер': 11,\n",
       " 'ул': 974,\n",
       " 'самойловой': 1,\n",
       " 'д': 908,\n",
       " 'радостью': 32,\n",
       " 'купим': 19,\n",
       " 'ваши': 197,\n",
       " 'игры': 167,\n",
       " 'консолей': 5,\n",
       " 'sony': 147,\n",
       " 'nintendo': 14,\n",
       " 'культиватор': 4,\n",
       " 'lemken': 2,\n",
       " 'метровый': 3,\n",
       " 'органы': 2,\n",
       " 'идеальное': 309,\n",
       " 'продажа': 403,\n",
       " 'детские': 393,\n",
       " 'туфли': 568,\n",
       " 'противотуманная': 22,\n",
       " 'фара': 260,\n",
       " 'правая': 281,\n",
       " 'диодная': 5,\n",
       " 'hyunda': 2,\n",
       " 'запчасть': 157,\n",
       " 'hyundai': 309,\n",
       " 'solaris': 57,\n",
       " 'рестайл': 9,\n",
       " 'oem': 162,\n",
       " '922024l700': 2,\n",
       " 'слом': 3,\n",
       " 'крепл': 2,\n",
       " 'марка': 221,\n",
       " 'хендай': 132,\n",
       " 'хундай': 44,\n",
       " 'солярис': 47,\n",
       " 'рестайлинг': 163,\n",
       " 'соларис': 5,\n",
       " 'сломано': 16,\n",
       " 'крепление': 100,\n",
       " 'требуется': 141,\n",
       " 'nokia': 19,\n",
       " 'продаю': 1485,\n",
       " 'телефон': 337,\n",
       " 'рабочй': 1,\n",
       " 'потертости': 40,\n",
       " 'зарядного': 8,\n",
       " 'устройства': 78,\n",
       " 'samsung': 249,\n",
       " 'syncmaster': 3,\n",
       " '941mp': 1,\n",
       " 'исползовать': 1,\n",
       " 'монитор': 124,\n",
       " 'одновременно': 30,\n",
       " 'компьютером': 8,\n",
       " 'вопросам': 201,\n",
       " 'verso': 8,\n",
       " 'коллектор': 80,\n",
       " 'впускной': 52,\n",
       " '171200t040': 1,\n",
       " '1028856s01': 1,\n",
       " 'применимость': 20,\n",
       " 'avensis': 14,\n",
       " 'после': 675,\n",
       " '2009г': 15,\n",
       " '2zrfae': 1,\n",
       " 'auris': 18,\n",
       " 'zre152': 1,\n",
       " 'zre152h': 1,\n",
       " 'zrt271': 1,\n",
       " 'zgr21': 1,\n",
       " 'городу': 238,\n",
       " 'датчик': 235,\n",
       " 'парковки': 64,\n",
       " 'bmw': 436,\n",
       " 'серия': 115,\n",
       " 'e39': 18,\n",
       " 'бмв': 180,\n",
       " 'детали': 600,\n",
       " 'штуки': 107,\n",
       " 'имеются': 224,\n",
       " 'другие': 799,\n",
       " 'этот': 182,\n",
       " 'автомобиль': 754,\n",
       " 'авторазбор': 127,\n",
       " 'иномарок': 64,\n",
       " 'саратове': 8,\n",
       " 'подержанные': 6,\n",
       " 'автозапчасти': 80,\n",
       " 'деталей': 204,\n",
       " 'посмотреть': 301,\n",
       " 'пройдите': 3,\n",
       " 'авангард': 3,\n",
       " 'азина': 3,\n",
       " 'саратову': 6,\n",
       " 'любая': 105,\n",
       " 'форма': 179,\n",
       " 'оплаты': 333,\n",
       " 'рф': 721,\n",
       " 'везем': 2,\n",
       " 'каждый': 271,\n",
       " 'день': 715,\n",
       " 'прямо': 235,\n",
       " 'сейчас': 370,\n",
       " 'полным': 81,\n",
       " 'списком': 14,\n",
       " 'товаров': 395,\n",
       " 'ознакомиться': 89,\n",
       " 'ссылке': 61,\n",
       " 'ниже': 269,\n",
       " 'арматура': 26,\n",
       " 'класс': 273,\n",
       " 'в600с': 1,\n",
       " '12мм': 11,\n",
       " 'строительстве': 17,\n",
       " 'особенно': 29,\n",
       " 'монолитном': 2,\n",
       " 'прочность': 21,\n",
       " 'конструкции': 44,\n",
       " 'имеет': 262,\n",
       " 'решающее': 1,\n",
       " 'значение': 9,\n",
       " 'металлопрокат': 14,\n",
       " 'устойчив': 11,\n",
       " 'изгибам': 1,\n",
       " 'растяжениям': 1,\n",
       " 'обладает': 50,\n",
       " 'вязкостью': 1,\n",
       " 'позволяет': 255,\n",
       " 'конструкциям': 1,\n",
       " 'выдерживать': 2,\n",
       " 'высокие': 59,\n",
       " 'нагрузки': 41,\n",
       " 'допуская': 1,\n",
       " 'разрушения': 2,\n",
       " 'зданий': 20,\n",
       " 'сооружений': 8,\n",
       " 'огромный': 232,\n",
       " 'выбор': 679,\n",
       " 'металлопроката': 12,\n",
       " 'доставкой': 100,\n",
       " 'наличный': 192,\n",
       " 'безналичный': 201,\n",
       " 'расчет': 212,\n",
       " 'нам': 380,\n",
       " 'банкноты': 11,\n",
       " 'узбекистана': 3,\n",
       " 'узбекские': 7,\n",
       " 'сумы': 1,\n",
       " 'каждая': 69,\n",
       " 'купюра': 8,\n",
       " 'руб': 2147,\n",
       " 'монеты': 44,\n",
       " 'азербайджана': 1,\n",
       " 'пакистана': 1,\n",
       " 'бангладеш': 5,\n",
       " 'афганистана': 1,\n",
       " 'непала': 1,\n",
       " 'также': 994,\n",
       " 'альбомы': 4,\n",
       " 'бон': 1,\n",
       " 'монет': 16,\n",
       " 'телята': 5,\n",
       " 'откорм': 1,\n",
       " 'бычки': 3,\n",
       " 'мясные': 4,\n",
       " 'каждую': 48,\n",
       " 'неделю': 95,\n",
       " 'несколько': 266,\n",
       " 'свежих': 5,\n",
       " 'завозов': 1,\n",
       " 'оплата': 622,\n",
       " 'скот': 1,\n",
       " 'здоровый': 4,\n",
       " 'ухоженный': 40,\n",
       " 'упитанный': 1,\n",
       " 'крупных': 29,\n",
       " 'племзаводов': 1,\n",
       " 'племенного': 1,\n",
       " 'скота': 3,\n",
       " 'привитые': 3,\n",
       " 'карантенизированны': 1,\n",
       " 'всеми': 95,\n",
       " 'вет': 12,\n",
       " 'справками': 1,\n",
       " 'всем': 287,\n",
       " 'буду': 38,\n",
       " 'рада': 11,\n",
       " 'ответить': 80,\n",
       " 'можите': 3,\n",
       " 'звонить': 277,\n",
       " 'писать': 120,\n",
       " 'ман': 20,\n",
       " 'тгх': 2,\n",
       " 'man': 43,\n",
       " 'tgx': 5,\n",
       " 'н': 239,\n",
       " 'аналогом': 7,\n",
       " 'артикулов': 2,\n",
       " 'me81251016498': 1,\n",
       " 'br05ma021': 1,\n",
       " '4491102rmldem': 1,\n",
       " 'применима': 4,\n",
       " 'следующим': 9,\n",
       " 'авто': 782,\n",
       " 'tgs': 1,\n",
       " 'складе': 383,\n",
       " 'сообщайте': 20,\n",
       " 'бронировании': 5,\n",
       " 'грузовая': 38,\n",
       " 'разборка': 114,\n",
       " 'любые': 402,\n",
       " 'грузовых': 39,\n",
       " 'тгл': 6,\n",
       " 'тга': 6,\n",
       " 'тгс': 1,\n",
       " 'склад': 216,\n",
       " 'россии': 1279,\n",
       " 'европе': 22,\n",
       " 'поэтому': 220,\n",
       " 'наличию': 89,\n",
       " 'необходимости': 141,\n",
       " 'отправляем': 362,\n",
       " 'месяц': 186,\n",
       " 'скажите': 22,\n",
       " 'вы': 1276,\n",
       " 'дадим': 13,\n",
       " 'скидку': 144,\n",
       " 'точно': 72,\n",
       " 'склоподьемника': 1,\n",
       " 'прямая': 33,\n",
       " 'труба': 69,\n",
       " 'выхлопа': 9,\n",
       " 'диски': 507,\n",
       " 'коробка': 456,\n",
       " 'хрустит': 3,\n",
       " 'кулиса': 37,\n",
       " 'калина': 19,\n",
       " 'короткоходка': 1,\n",
       " 'освежалась': 1,\n",
       " 'лаком': 23,\n",
       " 'назад': 113,\n",
       " 'бодрый': 4,\n",
       " 'реальному': 66,\n",
       " 'покупателю': 89,\n",
       " 'капота': 127,\n",
       " 'головка': 41,\n",
       " 'блока': 103,\n",
       " '2200042a20': 2,\n",
       " 'd4bh': 9,\n",
       " '4d56': 7,\n",
       " 'tcl': 4,\n",
       " 'цилиндров': 37,\n",
       " '42a20': 3,\n",
       " 'сборе': 248,\n",
       " 'клапанами': 2,\n",
       " 'распредвалом': 1,\n",
       " 'двигателей': 31,\n",
       " 'tci': 1,\n",
       " 'утопленные': 1,\n",
       " 'гбц': 70,\n",
       " '2200042а20': 2,\n",
       " 'двс': 210,\n",
       " 'производства': 178,\n",
       " 'nomparts': 1,\n",
       " 'ю': 27,\n",
       " 'корея': 34,\n",
       " 'устанавливается': 95,\n",
       " 'mitsubishi': 211,\n",
       " 'комплектация': 321,\n",
       " 'можете': 732,\n",
       " 'купить': 285,\n",
       " 'нашей': 166,\n",
       " 'компании': 453,\n",
       " 'низкой': 47,\n",
       " 'цене': 243,\n",
       " 'посредников': 74,\n",
       " 'оперативно': 35,\n",
       " 'отправим': 136,\n",
       " 'любой': 962,\n",
       " 'регион': 222,\n",
       " 'устанавливаестя': 1,\n",
       " 'galloper': 3,\n",
       " 'галлопер': 2,\n",
       " 'libero': 1,\n",
       " 'либеро': 1,\n",
       " 'h1': 9,\n",
       " 'н1': 1,\n",
       " 'h100': 1,\n",
       " 'н100': 1,\n",
       " 'starex': 18,\n",
       " 'старекс': 3,\n",
       " 'terracan': 3,\n",
       " 'терракан': 2,\n",
       " 'delica': 4,\n",
       " 'мицубиси': 39,\n",
       " 'делика': 4,\n",
       " 'pajero': 43,\n",
       " 'паджеро': 15,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokens['сапоги'] == 454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ выведите 10 самых частотных и 10 самых редких токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('iqmac', 1),\n",
       " ('ядерным', 1),\n",
       " ('ресурсоемкие', 1),\n",
       " ('быстродействием', 1),\n",
       " ('кинематографическому', 1),\n",
       " ('ирис', 1),\n",
       " ('саженец', 1),\n",
       " ('корень', 1),\n",
       " ('зубчаниновка', 1),\n",
       " ('боярышник', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tokens.items(), key=lambda x:x[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('в', 28337),\n",
       " ('и', 21714),\n",
       " ('на', 19465),\n",
       " ('с', 12860),\n",
       " ('по', 10000),\n",
       " ('для', 9627),\n",
       " ('не', 6045),\n",
       " ('до', 4541),\n",
       " ('от', 4482),\n",
       " ('состоянии', 3981)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tokens.items(), key=lambda x:x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ оставьте в словаре только топ 10000 самых частотных токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=sorted(tokens.items(), key=lambda x:x[1], reverse=True)[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in tokens:\n",
    "    if re.match(r'.*\\d.*', x):\n",
    "        print(x, y) #Проверка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['в',\n",
       " 'и',\n",
       " 'на',\n",
       " 'с',\n",
       " 'по',\n",
       " 'для',\n",
       " 'не',\n",
       " 'до',\n",
       " 'от',\n",
       " 'состоянии',\n",
       " 'у',\n",
       " 'за',\n",
       " 'все',\n",
       " 'размер',\n",
       " 'без']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_common_words = [x for x, y in tokens]\n",
    "most_common_words[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word: index for index, word in enumerate(most_common_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index['состоянии']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ реализуйте функцию, которая предложение переводит в вектор из чисел. То есть каждому слову из словаря сопоставляется количество раз, которое оно встретилось в предложении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess2(text: str) -> str:\n",
    "    text = preprocess(text)\n",
    "    text = re.sub(r'\\W|\\b\\d+\\b',' ', text)\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bow(text: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Возвращает вектор, где для каждого слова из most_common\n",
    "    указано количество его употреблений\n",
    "    input: строка\n",
    "    output: вектор размерности словаря\n",
    "    \"\"\"\n",
    "    sentence_tokens = word_tokenize(preprocess2(text))\n",
    "    sent_vec = np.zeros(len(most_common_words))\n",
    "    for token in sentence_tokens:\n",
    "        if token in word_to_index:\n",
    "            sent_vec[word_to_index[token]] += 1\n",
    "    return sent_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 купить\n",
      "1.0 чайник\n"
     ]
    }
   ],
   "source": [
    "vector = text_to_bow('купить чайник купить ВШЭ') # Проверка\n",
    "for i in range(len(vector)):\n",
    "    if vector[i]:\n",
    "        print(vector[i], most_common_words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ а теперь реализуйте функцию, которая преобразует наш датасет и для каждого текста из description сопоставляет вектор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def items_to_bow(items: np.array) -> np.array:\n",
    "    \"\"\" Для каждого товара возвращает вектор его bow \"\"\"\n",
    "    # Давайте для начала попробуем строить bow только из description товара\n",
    "    descriptions = items[:, 1]\n",
    "    vectors = np.zeros((len(items), len(most_common_words)))\n",
    "    for i, desc in tqdm(enumerate(descriptions)):\n",
    "        vectors[i] = text_to_bow(desc)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "wwOZaEpMSQsZ",
    "outputId": "8a30c3af-3517-42bd-a5f3-36206b4b264a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21000it [00:10, 2002.69it/s]\n",
      "9000it [00:04, 1882.95it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_bow = items_to_bow(X_train)\n",
    "X_test_bow = items_to_bow(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# чтобы видеть проход по итерациям можно использовать библиотеку tqdm\n",
    "# она работает примерно так\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJVLS8Fs3CeT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJoXiCWI7VF5"
   },
   "source": [
    "### Логистическая регрессия и SVC (0.5 балла)\n",
    "\n",
    "\n",
    "Теперь описание каждого товара представлено, как точка в многомерном пространстве.\n",
    "Очень важно запомнить эту идею: дальше мы будем рассматривать разные способы перехода от текста к точке в пространстве.\n",
    "\n",
    "Для BOW каждое измерение в пространстве -- какое-то слово.\n",
    "Мы предполагаем, что текст описывается набором каких-то популярных слов, которые в нём встречаются, а близкие по смыслу тексты будут использовать одинаковые слова.\n",
    "\n",
    "Обучите логистическую регрессию и SVC с базовыми параметрами.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "Ky3HV1rTSS9L",
    "outputId": "612a5f0d-76bd-44f4-eeeb-63b517443797"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LogisticRegression()\n",
    "lr_model = lr.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.704"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(lr_model.predict(X_test_bow), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert accuracy_score(lr_model.predict(X_test_bow), y_test) > 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "-c46ZT0lvF6T",
    "outputId": "4b1cb34a-201b-4dc2-9155-fdb6919c6c08"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "SVM_classifier = SVC(C=0.01, kernel='linear')\n",
    "svc_model = SVM_classifier.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow[:500,:100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# не такое долгое https://datascience.stackexchange.com/questions/989/svm-using-scikit-learn-runs-endlessly-and-never-completes-execution\n",
    "from sklearn.svm import SVC\n",
    "SVM_classifier = SVC(C=0.01, kernel='linear')\n",
    "svc_model = SVM_classifier.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.694"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(svc_model.predict(X_test_bow), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert accuracy_score(svc_model.predict(X_test_bow), y_test) > 0.68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwKE57YZ1Hzn"
   },
   "source": [
    "### Модификация признаков (0.5 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ewMlxQezL6Ax"
   },
   "source": [
    "Добавьте title товара в bow с произвольным весом, как изменится качество?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def items_to_bow(items: np.array, column: int) -> np.array:\n",
    "    descriptions = items[:, column]\n",
    "    vectors = np.zeros((len(items), len(most_common_words)))\n",
    "    for i, desc in tqdm(enumerate(descriptions)):\n",
    "        vectors[i] = text_to_bow(desc)\n",
    "    return vectors\n",
    "\n",
    "def full_items_to_bow(items: np.array) -> np.array:\n",
    "    title_features = items_to_bow(items, 0)\n",
    "    description_features = items_to_bow(items, 1)\n",
    "    return np.concatenate([title_features, description_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21000it [00:06, 3432.51it/s]\n",
      "21000it [00:12, 1696.32it/s]\n",
      "9000it [00:01, 5221.51it/s]\n",
      "9000it [00:04, 1863.30it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_full_bow = full_items_to_bow(X_train)\n",
    "X_test_full_bow = full_items_to_bow(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7776666666666666"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model, metrics\n",
    "lr = linear_model.LogisticRegression()\n",
    "lr_model = lr.fit(X_train_full_bow, y_train)\n",
    "\n",
    "metrics.accuracy_score(lr_model.predict(X_test_full_bow), y_test)\n",
    "\n",
    "# Accuracy увеличился на 0,08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Db4TyqzxMnby"
   },
   "source": [
    "Нормализуйте данные (`sklearn.preprocessing.normalize`) перед обучением. Что станет с качеством и почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A8rVy6q1Mn4J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6998888888888889"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "lr_model = lr.fit(preprocessing.normalize(X_train_full_bow), y_train)\n",
    "\n",
    "metrics.accuracy_score(lr_model.predict(preprocessing.normalize(X_test_full_bow)), y_test)\n",
    "\n",
    "# Accuracy уменьшился на 0.6. Возможно, это из-за того, что нормализация происходит по всей выборке (а они\n",
    "# разные в случае train и test, и отношение TP/(TP+FN) уменьшается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvCAL3qGDByj"
   },
   "source": [
    "### Mystem (1 балл)\n",
    "\n",
    "Попробуйте обучиться, используя токенизатор mystem. Сравните качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hz38TqqRDY6-"
   },
   "outputs": [],
   "source": [
    "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
    "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
    "!cp mystem /bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60oQ-6UgDcLF"
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/nlpub/pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGvNHfVsDfhq"
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Красивая мама красиво мыла раму\"\n",
    "m = Mystem()\n",
    "lemmas = m.lemmatize(text)\n",
    "print(''.join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = Mystem()\n",
    "stemtokens = {}\n",
    "for text in senlist:\n",
    "    freq = lemmatizer.lemmatize(text)\n",
    "    for token in freq:\n",
    "        if token not in stemtokens.keys():\n",
    "            stemtokens[token] = 1\n",
    "        else:\n",
    "            stemtokens[token] += 1\n",
    "stemtokens=sorted(stemtokens.items(), key=lambda x:x[1], reverse=True)[:10000]   \n",
    "stem_most_common_words = [x for x, y in stemtokens]\n",
    "stem_word_to_index = {word: index for index, word in enumerate(stem_most_common_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text_to_bow(text: str) -> np.array:\n",
    "    lemmatizer = Mystem()\n",
    "    sentence_tokens = lemmatizer.lemmatize(text)\n",
    "    sent_vec = np.zeros(len(stem_most_common_words))\n",
    "    for token in sentence_tokens:\n",
    "        if token in stem_word_to_index:\n",
    "            sent_vec[stem_word_to_index[token]] += 1\n",
    "    return sent_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_items_to_bow(items: np.array) -> np.array:\n",
    "    modified = items[:,0] + \" \" + items[:, 1]\n",
    "    vectors = np.zeros((len(items), len(stem_most_common_words)))\n",
    "    for i, desc in enumerate(modified):\n",
    "        vectors[i] = stem_text_to_bow(desc)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stem = stem_items_to_bow(X_train)\n",
    "test_stem=stem_items_to_bow(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_stem' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-8ed17159056d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlr_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_stem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_bow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_stem' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LogisticRegression()\n",
    "lr_model = lr.fit(train_stem, y_train)\n",
    "accuracy_score(lr_model.predict(X_test_bow), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "SVM_classifier = SVC(C=0.01, kernel='linear')\n",
    "svc_model = SVM_classifier.fit(train_stem, y_train)\n",
    "accuracy_score(svc_model.predict(test_stem), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXbsPtpfoB7m"
   },
   "source": [
    "### TF-IDF (5 баллов)\n",
    "\n",
    "Не все слова полезны одинаково, давайте попробуем [взвесить](http://tfidf.com/) их, чтобы отобрать более полезные.\n",
    "\n",
    "\n",
    "> TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "> \n",
    "> IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "\n",
    "\n",
    "В sklearn есть TfidfVectorizer, но в этом задании его использовать нельзя. Для простоты посчитайте общий tf-idf для title и description (то есть каждому объекту надо сопоставить вектор, где как документ будет рассматриваться конкатенация title и description)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ составьте словарь, где каждому слову из изначального словаря будет стоять в соответствии количество документов, где это слово встретилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_number_documents = {}\n",
    "for sentence in senlist:\n",
    "    for word in set(word_tokenize(sentence)):\n",
    "        if word in word_to_index:\n",
    "            if word in word_to_number_documents:\n",
    "                word_to_number_documents[word] += 1\n",
    "            else:\n",
    "                word_to_number_documents[word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ реализуйте функцию, где тексту в соответствие ставится tf-idf вектор. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tfidf(text: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Возвращает вектор, где для каждого слова из most_common\n",
    "    указано количество его употреблений\n",
    "    input: строка\n",
    "    output: вектор размерности словаря\n",
    "    \"\"\"\n",
    "    sentence_tokens = word_tokenize(preprocess2(text))\n",
    "    sent_vec = np.zeros(len(most_common_words))\n",
    "    for token in sentence_tokens:\n",
    "        if token in word_to_index:\n",
    "            tf = 1 / len(sentence_tokens)\n",
    "            idf = np.log(len(senlist) / word_to_number_documents[token])\n",
    "            sent_vec[word_to_index[token]] += tf * idf\n",
    "    return sent_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61245232, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_tfidf('в купить в чайник') # Проверка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ а теперь реализуйте функцию, которая преобразует наш датасет и для каждого объекта сопоставляет вектор tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(senlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_number_documents['в']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = 1/4\n",
    "idf = np.log(42000 / 12339)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def items_to_tfidf(items: np.array) -> np.array:\n",
    "    \"\"\" \n",
    "    Для каждого товара возвращает его tfidf вектор\n",
    "    \"\"\"\n",
    "    modified = items[:,0] + \" \" + items[:, 1]\n",
    "    vectors = np.zeros((len(items), len(most_common_words)))\n",
    "    for i, desc in enumerate(modified):\n",
    "        vectors[i] = text_to_tfidf(desc)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = items_to_tfidf(X_train)\n",
    "X_test_tfidf = items_to_tfidf(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YFA-8kE1RHk"
   },
   "source": [
    "__Задание:__ обучите логистическую регрессию и SVC, оцените качество (accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ULrXsF1m5sU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7206666666666667"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LogisticRegression()\n",
    "lr_model = lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "accuracy_score(lr_model.predict(X_test_tfidf), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "SVM_classifier = SVC(C=0.01, kernel='linear')\n",
    "svc_model = SVM_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "accuracy_score(svc_model.predict(X_test_tfidf), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jFdy3lUFDsOr"
   },
   "source": [
    "### Hashing Vectorizer (1 балл)\n",
    "\n",
    "Попробуйте использовать `sklearn.feature_extraction.text.HashingVectorizer` для векторизации текстов.\n",
    "Обязательно оцените качество работы алгоритмов классификации с использованием новой векторизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<21000x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 665481 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hash_vec = HashingVectorizer(n_features=10000)\n",
    "modified = X_train[:,0] + \" \" + X_train[:, 1]\n",
    "texts_tokenized = [' '.join([w for w in word_tokenize(t) if w.isalpha()]) for t in modified]\n",
    "X = hash_vec.fit_transform(texts_tokenized)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21000, 10000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified2 = X_test[:,0] + \" \" + X_test[:, 1]\n",
    "texts_tokenized_test = [' '.join([w for w in word_tokenize(t) if w.isalpha()]) for t in modified2]\n",
    "X_test_hash=hash_vec.transform(texts_tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7067777777777777"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LogisticRegression()\n",
    "lr_model = lr.fit(X, y_train)\n",
    "\n",
    "accuracy_score(lr_model.predict(X_test_hash), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "SVM_classifier = SVC(C=0.01, kernel='linear')\n",
    "svc_model = SVM_classifier.fit(X, y_train)\n",
    "\n",
    "accuracy_score(svc_model.predict(X_test_hash), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQZ61xSsTpZI"
   },
   "source": [
    "### Word Vectors (3 балла)\n",
    "\n",
    "Давайте попробуем другой подход -- кажому слову сопоставим какой-то эмбеддинг (вектор).\n",
    "\n",
    "Вектора будут небольшой размерности. Таким образом мы снизим количество параметров в модели.\n",
    "\n",
    "Вектора мы возьмём уже готовые (обученные на текстах их интернета), так что наша модель будет знать некоторую дополнительную информацию о внешнем мире."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "id": "T38J27NcYGx5",
    "outputId": "57fa3a9f-13a3-4fa1-d13c-3c0c49a86a71"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/0x7oxso6x93efzj/ru.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zfse4xVbgMIr"
   },
   "outputs": [],
   "source": [
    "!tar -xzf ru.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sy2TXmQ2jZSY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "model = FastText.load_fasttext_format('ru.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# как мы видим, каждому слову данная модель сопоставляет вектор размерности 300\n",
    "\n",
    "print(model['привет'].shape)\n",
    "print(model['привет'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H49QR_jhjmCa"
   },
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Складывает вектора токенов строки sentence\n",
    "    \"\"\"\n",
    "    sentence_tokens = word_tokenize(preprocess2(sentence))\n",
    "    embedding = np.zeros(300)\n",
    "    for token in sentence_tokens:\n",
    "        if token in model:\n",
    "            embedding += model[token]\n",
    "        \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gj6U_hjtlllV"
   },
   "outputs": [],
   "source": [
    "assert np.allclose(sentence_embedding('сдаётся уютный , тёплый гараж для стартапов в ml')[::50],\n",
    "                   np.array([ 0.08189847,  0.07249198, -0.15601222,  0.03782297,  0.09215296, -0.23092946]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание:__ сделайте все то же, что в предыдущих пунктах -- реализуйте функцию, которая преобразует данные, а затем обучите логистическую регрессию и SVM, оцените качество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8tfhc-PFmGvu"
   },
   "outputs": [],
   "source": [
    "def word_vectors1(items: np.array, column: int) -> np.array:\n",
    "    descriptions = items[:, column]\n",
    "    vectors = np.zeros((len(items), 300))\n",
    "    for i, desc in tqdm(enumerate(descriptions)):\n",
    "        vectors[i] = sentence_embedding(desc)\n",
    "    return vectors\n",
    "\n",
    "def word_vectors2(items: np.array) -> np.array:\n",
    "    title_features = word_vectors1(items, 0)\n",
    "    description_features = word_vectors1(items, 1)\n",
    "    return np.concatenate([title_features, description_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21000it [00:05, 3577.09it/s]\n",
      "21000it [00:34, 611.20it/s]\n",
      "9000it [00:02, 3853.62it/s]\n",
      "9000it [00:13, 650.87it/s]\n"
     ]
    }
   ],
   "source": [
    "train_vec=word_vectors2(X_train)\n",
    "test_vec=word_vectors2(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr_model=linear_model.LogisticRegression()\n",
    "lr_model=lr_model.fit(train_vec, y_train)\n",
    "metrics.accuracy_score(lr_model.predict(test_vec), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_classifier = SVC(C=0.01, kernel='linear')\n",
    "svc_model = SVM_classifier.fit(train_vec, y_train)\n",
    "accuracy_score(svc_model.predict(test_vec), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cVEdlFostSnX"
   },
   "source": [
    "### Что дальше? (6 баллов)\n",
    "\n",
    "Для получения максимальной оценки вам нужно решить любые 2 пункта. Решение каждого пункта даст вам полтора балла:\n",
    "\n",
    "1. Реализовать N-Gram модели текстовой классификации (__1.5 балла__)\n",
    "\n",
    "2. Поработать с другими эмбеддингами для слов (например word2vec или GloVe) (__1.5 балла__)\n",
    "\n",
    "3. Другие способы токенизации (pymorphy2, spaCy) (__1.5 балла__)\n",
    "\n",
    "4. Добиться качества > 0.765 на тестовых данных (попробуйте другие токенизаторы, предобработку текста, и любые другие идеи, которые вам придут в голову) (__1.5 балла__)\n",
    "\n",
    "Снабжайте код пояснениями и графиками.\n",
    "Обязательно необходимо написать вывод по каждому пункту, который вы реализуете."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Качество > 0.765 достигнуто с первым алгоритмом\n",
    "### Способ токенизации pymorphy\n",
    "Работаем так же, как с Mystem, но на вход принимаются только слова, поэтому предварительно обрабатываем Tweet Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "pymorphy2_analyzer = MorphAnalyzer()\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "def lemma_py(sentence):\n",
    "    tw = TweetTokenizer()\n",
    "    sentence=tw.tokenize(sentence)\n",
    "    for i in range (len(sentence)):\n",
    "        result= pymorphy2_analyzer.parse(sentence[i])\n",
    "        sentence[i]=result[0].normal_form\n",
    "        i+=1\n",
    "    return sentence\n",
    "pytokens = {}\n",
    "for i in senlist:\n",
    "    freq = lemma_py(text)\n",
    "    for token in freq:\n",
    "        if token not in pytokens.keys():\n",
    "            pytokens[token] = 1\n",
    "        else:\n",
    "            pytokens[token] += 1\n",
    "pytokens=sorted(pytokens.items(), key=lambda x:x[1], reverse=True)[:10000]   \n",
    "py_most_common_words = [x for x, y in pytokens]\n",
    "py_word_to_index = {word: index for index, word in enumerate(py_most_common_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_text_to_bow(text: str) -> np.array:\n",
    "    sentence_tokens = lemma_py(text)\n",
    "    sent_vec = np.zeros(len(py_most_common_words))\n",
    "    for token in sentence_tokens:\n",
    "        if token in py_word_to_index:\n",
    "            sent_vec[py_word_to_index[token]] += 1\n",
    "    return sent_vec\n",
    "def py_items_to_bow(items: np.array) -> np.array:\n",
    "    modified = items[:,0] + \" \" + items[:, 1]\n",
    "    vectors = np.zeros((len(items), len(py_most_common_words)))\n",
    "    for i, desc in enumerate(modified):\n",
    "        vectors[i] = py_text_to_bow(desc)\n",
    "    return vectors\n",
    "X_train_py = py_items_to_bow(X_train)\n",
    "X_test_py = py_items_to_bow(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LogisticRegression()\n",
    "lr_model = lr.fit(X_train_py, y_train)\n",
    "\n",
    "print(accuracy_score(lr_model.predict(X_test_py), y_test))\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "SVM_classifier = SVC(C=0.01, kernel='linear')\n",
    "svc_model = SVM_classifier.fit(X_train_py, y_train)\n",
    "\n",
    "print(accuracy_score(svc_model.predict(X_test_py), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-граммы. \n",
    "Берем биграммы и ищем самые популярные по тексту. Так же создаем словарь самых часто встречающихся и возвращаем вектор, где для каждой биграммы из словаря указано количество употреблений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import ngrams\n",
    "def ngrammodel(items: np.array, n: int) -> list:\n",
    "    return[list(ngrams(i.split(),n)) for i in items]\n",
    "X_train_n= ngrammodel(train_stem, 2)\n",
    "X_test_n= ngrammodel(test_stem, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_n={}\n",
    "for item in X_train_n:\n",
    "    for i in item:\n",
    "        if i not in ngram_n:\n",
    "            ngram_n[i]=1\n",
    "        else:\n",
    "            ngram_n[i]+=1\n",
    "most_common_ngram={}\n",
    "for(key, value) in sorted(ngram_n.items(), key=lambda x: x[1])[-10000:]:\n",
    "    most_common_ngram[key]=value\n",
    "most_common_ngram\n",
    "\n",
    "\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_items_to_bow(items:np.array) -> np.array:\n",
    "    def n_str_to_bow(text:str) -> np.array:\n",
    "        d=dict(zip(most_common_ngram.keys(), np.zeros(len(most_common_ngram))))\n",
    "        text=word_tokenize(text)\n",
    "        for word1, word2 in zip(text[:-1], text[1:]):\n",
    "            if (word1, word2) in d.keys():\n",
    "                d[(word1, word2)]+=1\n",
    "        return list(d.values())\n",
    "    bow=[]\n",
    "    for i in items:\n",
    "        i = n_str_to_bow(i)\n",
    "        bow.append(i)\n",
    "    return np.array(bow)                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nn = n_items_to_bow(train_stem)\n",
    "X_test_nn = n_items_to_bow(test_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lr = linear_model.LogisticRegression()\n",
    "lr_model = lr.fit(X_train_nn, y_train)\n",
    "\n",
    "print(accuracy_score(lr_model.predict(X_test_nn), y_test))\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "SVM_classifier = SVC(C=0.01, kernel='linear')\n",
    "svc_model = SVM_classifier.fit(X_train_nn, y_train)\n",
    "\n",
    "print(accuracy_score(svc_model.predict(X_test_nn), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работаем с эмбеддингом Word2Vec\n",
    "Смысл такой же, как с Fasttext, но в итоге accuracy выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=FastText.load_fasttext_format('word2vec-ruscorpora-300')\n",
    "keys=[i.split('_')[0] for i in list(model.vocab.keys())]\n",
    "model=dict(zip(keys, model.vectors))\n",
    "\n",
    "train_wordvec=word_vectors2(train_stem)\n",
    "test_wordvec=word_vectors2(test_stem)\n",
    "\n",
    "lr_model=LogisticRegression()\n",
    "lr_model=lr_model.fit(train_wordvec, y_train)\n",
    "print(accuracy_score(lr_model.predict(test_wordvec), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_classifier = SVC(C=0.01, kernel='linear')\n",
    "svc_model = SVM_classifier.fit(train_wordvec, y_train)\n",
    "accuracy_score(svc_model.predict(test_wordvec), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
